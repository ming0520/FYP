[Script Info]
; Script generated by Aegisub 3.2.2
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: None

[Aegisub Project Garbage]
Audio File: E:/Users/User/Documents/Adobe/Premiere Pro/14.0/MSCP-Trans.mp3
Scroll Position: 146
Active Line: 155

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,20,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.44,0:00:01.76,Default,,0,0,0,,Hello Guys 
Dialogue: 0,0:00:01.94,0:00:04.68,Default,,0,0,0,,I'm here to propose an auto editing app
Dialogue: 0,0:00:04.68,0:00:06.92,Default,,0,0,0,,Hello, my name is Tan Zhong Ming
Dialogue: 0,0:00:06.94,0:00:10.12,Default,,0,0,0,,Just a reminder, It is supposed to be automatic
Dialogue: 0,0:00:10.12,0:00:12.70,Default,,0,0,0,,but actually you have to push this button.
Dialogue: 0,0:00:12.70,0:00:14.70,Default,,0,0,0,,Let us do a quick recap.
Dialogue: 0,0:00:14.70,0:00:17.40,Default,,0,0,0,,The first problem we mentioned in last time is
Dialogue: 0,0:00:17.40,0:00:19.98,Default,,0,0,0,,we need to  remove stuttered speech manually
Dialogue: 0,0:00:19.98,0:00:22.82,Default,,0,0,0,,But this time i do a little bit modification
Dialogue: 0,0:00:22.82,0:00:27.00,Default,,0,0,0,,My second problem is improving the accuracy of subtitles.
Dialogue: 0,0:00:27.00,0:00:29.52,Default,,0,0,0,,This can be done by using several techniques such as
Dialogue: 0,0:00:29.52,0:00:32.36,Default,,0,0,0,,denoising, audio pre-process and deep speech.
Dialogue: 0,0:00:32.36,0:00:37.82,Default,,0,0,0,,Audio pre-processing includes pre-emphasis, normalization, amplification and etc.
Dialogue: 0,0:00:37.82,0:00:41.88,Default,,0,0,0,,But denoising is not necessary is audio pre-processing,
Dialogue: 0,0:00:41.88,0:00:45.84,Default,,0,0,0,,Because it can be done by using AI to remove the background noise.
Dialogue: 0,0:00:45.84,0:00:48.10,Default,,0,0,0,,This has been done by Nvidia,
Dialogue: 0,0:00:48.10,0:00:51.12,Default,,0,0,0,,they publish a new feature for their GPU
Dialogue: 0,0:00:51.12,0:00:54.20,Default,,0,0,0,,which is able to remove the background noise real time
Dialogue: 0,0:00:54.20,0:00:57.10,Default,,0,0,0,,by using the latest RTX series GPU.
Dialogue: 0,0:00:57.10,0:01:01.24,Default,,0,0,0,,Next, deep speech has a good potential to become the solution.
Dialogue: 0,0:01:01.24,0:01:03.46,Default,,0,0,0,,We will talk about this later.
Dialogue: 0,0:01:03.46,0:01:05.68,Default,,0,0,0,,So, the scope also must change,
Dialogue: 0,0:01:05.68,0:01:06.88,Default,,0,0,0,,here is my latest scope.
Dialogue: 0,0:01:06.88,0:01:12.18,Default,,0,0,0,,First we need to perform pre-processing techniques to audio data set for better sound quality
Dialogue: 0,0:01:12.18,0:01:17.88,Default,,0,0,0,,Sceond, build and train an Artificial Neural Network (ANN) for the stuttered speech classification.
Dialogue: 0,0:01:17.88,0:01:23.44,Default,,0,0,0,,Third, we need to build and train an End-to-end deep learning model for Deep Speech
Dialogue: 0,0:01:23.44,0:01:27.54,Default,,0,0,0,,The deep speech will implement in auto subtitle algorithm.
Dialogue: 0,0:01:27.62,0:01:34.38,Default,,0,0,0,,Fourth, we need to develop an application and algorithm for Android to automatically remove the stuttered speech.
Dialogue: 0,0:01:34.38,0:01:42.12,Default,,0,0,0,,Fifth, we need to implement audio pre-processing technique to improve the accuracy of speech recognition to generate subtitle.
Dialogue: 0,0:01:42.12,0:01:47.42,Default,,0,0,0,,The last one, we need to develop an improved version of auto subtitle generator.
Dialogue: 0,0:01:47.42,0:01:48.60,Default,,0,0,0,,Why we need this app.
Dialogue: 0,0:01:48.60,0:01:51.20,Default,,0,0,0,,We mentioned removing the stuttered speech by humans
Dialogue: 0,0:01:51.20,0:01:52.98,Default,,0,0,0,,is slow and inefficient. 
Dialogue: 0,0:01:52.98,0:01:57.16,Default,,0,0,0,,The editor is required to remove and import one by one from the library.
Dialogue: 0,0:01:57.16,0:02:00.20,Default,,0,0,0,,Apart from that, editors need to denoise, transcribe and
Dialogue: 0,0:02:00.20,0:02:04.54,Default,,0,0,0,,process the audio manually to get better results from the subtitle generator.
Dialogue: 0,0:02:04.54,0:02:08.12,Default,,0,0,0,,The proposed app has the potential to detect and remove the stuttered speech
Dialogue: 0,0:02:08.12,0:02:12.84,Default,,0,0,0,,automatically in batch. Second it has better accuracy to auto generate subtitles.
Dialogue: 0,0:02:12.84,0:02:17.62,Default,,0,0,0,,The processing time for the current video editing process is high, but the app is low.
Dialogue: 0,0:02:17.62,0:02:21.84,Default,,0,0,0,,That means it is faster. Apart from that the human involvement for the proposed app
Dialogue: 0,0:02:21.84,0:02:24.56,Default,,0,0,0,,is significantly reduced. Why do we need this app?  
Dialogue: 0,0:02:24.56,0:02:27.92,Default,,0,0,0,,From my study, 4 out of 6 of my friends are beginner level.
Dialogue: 0,0:02:28.08,0:02:32.14,Default,,0,0,0,,So, they hope there is app like this with the subtitle generator.
Dialogue: 0,0:02:32.14,0:02:34.66,Default,,0,0,0,,Now, let me share what I found on the internet.
Dialogue: 0,0:02:34.66,0:02:37.76,Default,,0,0,0,,The first one of course is comparative study.
Dialogue: 0,0:02:37.76,0:02:43.04,Default,,0,0,0,,Dataset is always the most important step in machine learning. No data no talk.
Dialogue: 0,0:02:43.04,0:02:46.88,Default,,0,0,0,,There is a lot of different language dataset for stuttered speech such as
Dialogue: 0,0:02:46.88,0:02:52.42,Default,,0,0,0,,UCLASS, Indian Regional Language, Polish Language, German Language and etc.
Dialogue: 0,0:02:52.42,0:02:57.02,Default,,0,0,0,,For the UCLASS database, they have version 1 and version2.
Dialogue: 0,0:02:57.02,0:03:01.40,Default,,0,0,0,,The UCLASS database s collected by University College of London.
Dialogue: 0,0:03:01.40,0:03:07.34,Default,,0,0,0,,The full name for dataset is University College London’s Archive of Stuttered Speech. 
Dialogue: 0,0:03:07.34,0:03:14.24,Default,,0,0,0,,First version only includes monolog, the second version includes monolog, reading and conversation.
Dialogue: 0,0:03:14.24,0:03:20.16,Default,,0,0,0,,The age for the version 1 is between 5 years 4 month until 47 years old.
Dialogue: 0,0:03:20.16,0:03:22.70,Default,,0,0,0,,For the version 2, we have more choices.
Dialogue: 0,0:03:22.70,0:03:27.22,Default,,0,0,0,, Version 2 is 7 years old 10 month until 20 years 7 month.
Dialogue: 0,0:03:27.22,0:03:34.30,Default,,0,0,0,,Version 1 includes 120 male and 18 females and version 2 has more options.
Dialogue: 0,0:03:34.30,0:03:39.44,Default,,0,0,0,,Here is the summary of the several previous collections of research work on
Dialogue: 0,0:03:39.44,0:03:43.42,Default,,0,0,0,,stuttered speech recognition. We can see in the early age,
Dialogue: 0,0:03:43.42,0:03:48.38,Default,,0,0,0,,the database they used is non standard. The size of the sample and speaker are also non fixed.
Dialogue: 0,0:03:48.38,0:03:53.64,Default,,0,0,0,,The features they used also are non standardized and required a lot of process on audio.
Dialogue: 0,0:03:53.64,0:04:01.26,Default,,0,0,0,,But they still get pretty good results, the accuracy they get is range from 73.25% to 92%.
Dialogue: 0,0:04:01.98,0:04:06.56,Default,,0,0,0,,They used ANN, HMM and SVM as the classifier. 
Dialogue: 0,0:04:06.56,0:04:10.06,Default,,0,0,0,,This is the summary for research within 10 years.
Dialogue: 0,0:04:10.06,0:04:14.00,Default,,0,0,0,,We can see almost everyone uses the UCLASS dataset as their data. 
Dialogue: 0,0:04:14.00,0:04:17.24,Default,,0,0,0,,This will provide a very good baseline for our project..
Dialogue: 0,0:04:17.24,0:04:22.28,Default,,0,0,0,,Most of them use MFCC features and k-NN architecture. 
Dialogue: 0,0:04:22.28,0:04:25.74,Default,,0,0,0,,We can see the accuracy does improve from previous work.
Dialogue: 0,0:04:25.74,0:04:29.10,Default,,0,0,0,,But the downside of MFCC is sensitive to noise.
Dialogue: 0,0:04:29.10,0:04:35.58,Default,,0,0,0,,This is the second literature review, the paper talking about the detection and analysis of stuttered speech.
Dialogue: 0,0:04:35.58,0:04:41.40,Default,,0,0,0,,They use MFCC technique to extract the feature so no pre-emphasis is required. 
Dialogue: 0,0:04:41.40,0:04:44.92,Default,,0,0,0,,Then classify the stuttered speech by using SVM.
Dialogue: 0,0:04:44.92,0:04:50.50,Default,,0,0,0,,They obtain 96.67% of accuracy with 10-12 coefficient.
Dialogue: 0,0:04:50.50,0:04:55.44,Default,,0,0,0,,Third paper that I read is speech recognition and correction of stuttered speech.
Dialogue: 0,0:04:55.44,0:05:01.88,Default,,0,0,0,,The goal of this paper is to develop a system that is able to identify and correct the stuttered speech.
Dialogue: 0,0:05:01.88,0:05:07.10,Default,,0,0,0,,Two algorithms are required to make it work. First is the algorithm to remove prolonged stuttered speech.
Dialogue: 0,0:05:07.10,0:05:12.02,Default,,0,0,0,,This algorithm will obtain the maximum amplitude as input for the neural network.
Dialogue: 0,0:05:12.02,0:05:16.88,Default,,0,0,0,,Then the neural network will output a proper threshold to remove the prolongation.
Dialogue: 0,0:05:16.88,0:05:20.14,Default,,0,0,0,,Second is removing the repetition of stuttered speech.
Dialogue: 0,0:05:20.14,0:05:25.78,Default,,0,0,0,,This algorithm will use Text-to-Speech technique to remove repetition of stuttered speech.
Dialogue: 0,0:05:25.78,0:05:29.94,Default,,0,0,0,,They chunk the samples into 6 seconds segments for training.
Dialogue: 0,0:05:29.94,0:05:36.50,Default,,0,0,0,,The processing time for this system is around 5- 8 seconds and they obtain 86% of accuracy.
Dialogue: 0,0:05:36.58,0:05:38.72,Default,,0,0,0,,Last one is deep speech 2.
Dialogue: 0,0:05:38.72,0:05:40.72,Default,,0,0,0,,Also known as DS2.
Dialogue: 0,0:05:40.72,0:05:42.72,Default,,0,0,0,,This research is done by Baidu
Dialogue: 0,0:05:42.72,0:05:47.78,Default,,0,0,0,,The goal of deep speech is to predict multiple languages by only using 1 model.
Dialogue: 0,0:05:47.78,0:05:52.90,Default,,0,0,0,,This is because the current ASR Automatic Speech Recognition is overcomplicated.
Dialogue: 0,0:05:52.90,0:05:57.34,Default,,0,0,0,,Because the current system divides the word into phonemes, syllables and more.
Dialogue: 0,0:05:57.34,0:06:02.72,Default,,0,0,0,,This will make the current system face the problem if they need to implement another new language.
Dialogue: 0,0:06:02.72,0:06:07.82,Default,,0,0,0,,So, they hope to develop only one engine applicable to all the languages.
Dialogue: 0,0:06:07.82,0:06:13.54,Default,,0,0,0,,And they hope to solve other issues too, such as accented speech, noisy environment and more.
Dialogue: 0,0:06:13.54,0:06:18.08,Default,,0,0,0,,This is the architecture of the DS2. Total 11 layers.
Dialogue: 0,0:06:18.08,0:06:24.02,Default,,0,0,0,,They have 3 convolution layers, 7 recurrent layers and 1 fully connected layer.
Dialogue: 0,0:06:24.02,0:06:30.86,Default,,0,0,0,,The fully connected layer is responsible to calculate the probability distribution and provide output to the CTC layer.
Dialogue: 0,0:06:30.86,0:06:34.94,Default,,0,0,0,,and Batch Normalization is implement for all the layer.
Dialogue: 0,0:06:34.94,0:06:42.36,Default,,0,0,0,,The data they used for English in the experiment is WSJ, Switchboard, Fisher, LibriSpeech, and Baidu.
Dialogue: 0,0:06:42.36,0:06:44.72,Default,,0,0,0,,Baidu dataset only for internal usage.
Dialogue: 0,0:06:44.72,0:06:50.56,Default,,0,0,0,,So they have a total eleven thousand nine hundred forty hours of speech data.
Dialogue: 0,0:06:50.56,0:06:56.12,Default,,0,0,0,,WSJ, Switchboard, Fisher you can get from Linguistic Data Consortium.
Dialogue: 0,0:06:56.12,0:06:59.32,Default,,0,0,0,,LibriSpeech is the only free dataset in here.
Dialogue: 0,0:06:59.32,0:07:05.36,Default,,0,0,0,,They use 95minutes data per epoch for English and 25 minute per epoch for mandarin.
Dialogue: 0,0:07:05.36,0:07:09.72,Default,,0,0,0,,For data augmentation, they added noise to 40% of the dataset.
Dialogue: 0,0:07:09.72,0:07:12.12,Default,,0,0,0,,Next is the most interesting thing.
Dialogue: 0,0:07:12.12,0:07:16.38,Default,,0,0,0,,The read speech DS2 outperforms humans.
Dialogue: 0,0:07:16.38,0:07:22.20,Default,,0,0,0,,We can see some of the Word Error Rate (WER) is better than humans.
Dialogue: 0,0:07:22.20,0:07:25.46,Default,,0,0,0,,But accented speech is close to human performance.
Dialogue: 0,0:07:25.46,0:07:29.12,Default,,0,0,0,,Unfortunately noisy speech is worse than humans.
Dialogue: 0,0:07:29.12,0:07:36.24,Default,,0,0,0,,This is because they use synthetic voices rather than real environment recording.
Dialogue: 0,0:07:36.24,0:07:41.60,Default,,0,0,0,,Following is the methodology that I choose is scrum. Why?
Dialogue: 0,0:07:41.60,0:07:45.24,Default,,0,0,0,,Because it is fast and supports incremental deliverable.
Dialogue: 0,0:07:45.24,0:07:49.34,Default,,0,0,0,,Next is user story makes me understand the user requirement.
Dialogue: 0,0:07:49.34,0:07:52.32,Default,,0,0,0,,Product backlog can ensure the quality of the outcome.
Dialogue: 0,0:07:52.32,0:07:55.92,Default,,0,0,0,,Software, this is important because I faced a problem.
Dialogue: 0,0:07:55.92,0:08:00.66,Default,,0,0,0,,This problem is not caused by human error. It is a compatibility issue.
Dialogue: 0,0:08:00.66,0:08:05.38,Default,,0,0,0,,The story is when I need to read the audio by using librosa library.
Dialogue: 0,0:08:05.38,0:08:10.46,Default,,0,0,0,,It is a python distribution that allows us to access the file in the directory.
Dialogue: 0,0:08:10.46,0:08:16.40,Default,,0,0,0,,“pip” and “npm” is famous and professional tools. But after I installed librosa library.
Dialogue: 0,0:08:16.40,0:08:20.92,Default,,0,0,0,,I cannot compile my python code. I reinstall over and over again,
Dialogue: 0,0:08:20.92,0:08:26.48,Default,,0,0,0,,check through all the github and stackoverflow changing the version. But still didn’t solve the issues.
Dialogue: 0,0:08:26.48,0:08:29.50,Default,,0,0,0,,The moment I wanted to reinstall the whole environment,
Dialogue: 0,0:08:29.50,0:08:34.82,Default,,0,0,0,,I saw many articles mentioning the issues when installing  the librosa by using pip.
Dialogue: 0,0:08:34.82,0:08:40.28,Default,,0,0,0,,So they have to manually install the librosa, download, unzip, and put it to the folder.
Dialogue: 0,0:08:40.28,0:08:43.62,Default,,0,0,0,,So I know it is the distribution software problem.
Dialogue: 0,0:08:43.62,0:08:48.54,Default,,0,0,0,,Then I download anaconda3, anaconda 3 also is a python distribution. Ahha,
Dialogue: 0,0:08:48.54,0:08:54.34,Default,,0,0,0,,then I solve the issues, my code doesn't have any problem, librosa also don't have any problem.
Dialogue: 0,0:08:54.34,0:08:59.96,Default,,0,0,0,,The problem is the python distribution. So I will use anaconda 3 as my development tools.
Dialogue: 0,0:08:59.96,0:09:05.96,Default,,0,0,0,,The minimum requirement to run tensorflow in python 3.5 or above. 
Dialogue: 0,0:09:05.96,0:09:08.14,Default,,0,0,0,,And to support tensorflow lite,
Dialogue: 0,0:09:08.14,0:09:12.72,Default,,0,0,0,,you need at least  support android 6.0 or API23 above.
Dialogue: 0,0:09:12.72,0:09:16.94,Default,,0,0,0,,Here is the new system workflow. It didn't change too much, just adding a new module.
Dialogue: 0,0:09:16.94,0:09:23.04,Default,,0,0,0,,I insert a deep speech module here to generate subtitles for us. Hope it can flip this industry.
Dialogue: 0,0:09:23.04,0:09:28.60,Default,,0,0,0,,Here is the activity diagram for traditional methods. It is very similar to the previous video.
Dialogue: 0,0:09:28.60,0:09:33.20,Default,,0,0,0,,Here is the proposed method. It is also very similar to the previous design.
Dialogue: 0,0:09:33.20,0:09:37.88,Default,,0,0,0,,But I added an auto subtitle generator by using a deep speech module over here.
Dialogue: 0,0:09:37.88,0:09:43.12,Default,,0,0,0,,This is the use case diagram. Users are able to record, edit video and edit the subtitle.
Dialogue: 0,0:09:43.12,0:09:49.92,Default,,0,0,0,,The special about this app is this app has a feature to remove the stuttered speech and auto generate subtitles for us.
Dialogue: 0,0:09:49.92,0:09:56.54,Default,,0,0,0,,These features will be included in edit video and edit subtitle.
Dialogue: 0,0:09:56.54,0:09:59.46,Default,,0,0,0,,This is the activity diagram for editing the video.
Dialogue: 0,0:10:19.48,0:10:21.58,Default,,0,0,0,,So let us talk about the app design.
Dialogue: 0,0:10:21.58,0:10:27.44,Default,,0,0,0,,The app will include three main screens which are home screen, edit video screen and edit subtitle screen.
Dialogue: 0,0:10:27.44,0:10:33.50,Default,,0,0,0,,The feature mentioned in above will present in “one button to solve all” way. First is the home screen.
Dialogue: 0,0:10:33.50,0:10:39.96,Default,,0,0,0,,The user is able to record directly from the app or choose to import the video for future process.
Dialogue: 0,0:10:39.96,0:10:42.96,Default,,0,0,0,,This is the most efficient and fastest way for users.
Dialogue: 0,0:10:42.96,0:10:47.34,Default,,0,0,0,,Here is the extent for the first screen when the user chooses to press import,
Dialogue: 0,0:10:47.34,0:10:52.52,Default,,0,0,0,,in this screen user is able to view the gallery and import the video from the gallery.
Dialogue: 0,0:10:52.52,0:10:56.88,Default,,0,0,0,,Third is edit videos. This screen will take most of the time for the user.
Dialogue: 0,0:10:56.88,0:11:02.82,Default,,0,0,0,,Users are able to play or pause the video here. The feature will represent indirectly on the app.
Dialogue: 0,0:11:02.82,0:11:05.74,Default,,0,0,0,,because this project is more to automate the process.
Dialogue: 0,0:11:05.74,0:11:09.70,Default,,0,0,0,,The main goal of this project is to remove the stuttered speech automatically.
Dialogue: 0,0:11:09.78,0:11:13.46,Default,,0,0,0,,so the video will be cut into segments and removed automatically, 
Dialogue: 0,0:11:13.46,0:11:16.46,Default,,0,0,0,,or the user can choose to remove the segment manually.
Dialogue: 0,0:11:16.52,0:11:23.18,Default,,0,0,0,,So, we can see over here, here is the video player and here is the segment that user can choose to remove.
Dialogue: 0,0:11:23.18,0:11:26.62,Default,,0,0,0,,Here is the play pause button, previous and next.
Dialogue: 0,0:11:26.62,0:11:31.96,Default,,0,0,0,,This is the subtitle screen. The subtitle will be auto generated and present on this screen. 
Dialogue: 0,0:11:31.96,0:11:38.06,Default,,0,0,0,,The user is able to adjust the start and end time, edit the subtitle and pause the video.
Dialogue: 0,0:11:38.14,0:11:41.02,Default,,0,0,0,,Subtitles will be auto generated on this screen.
Dialogue: 0,0:11:41.12,0:11:43.48,Default,,0,0,0,,Thank you very much. That is my presentation.
Dialogue: 0,0:11:43.48,0:11:45.48,Default,,0,0,0,,
